---
title: "Project: Basic RNN in PyTorch"
date: 2020-01-24
tags: [pytorch, python, machine learning, recurrent neural network, data science]
header:
    image: "/images/pytorch-basic-rnn/RNN-LSTM-GRU.png"
excerpt: "PyTorch, Basic RNN, Machine Learning"
mathjax: true
---

## Out with MNIST in with Sequential Integer Sequences
I don't like MNIST recurrent neural network tutorials as they unnecessarily increase the problems complexity for newcomers. Rather than using MNIST, we will use a basic sequential integer sequence to understand the inner-working of the RNN module in PyTorch.

### What does 'Sequential Integer Sequence' mean?
Basically, we generate a sequential sequence of integers 0, 1, 2, 3, ..., n.

### How does this relate to recurrent neural networks?
The application of this dataset to recurrent neural networks is straight-forward, we'll simply feed in the sequence and use the last integer as the label that needs to be predicted, for example: **input**: 0, 1, 2, 3 and **target**: 4. Pretty trivial.

### Code Implementation
#### Importing Libraries
Import required libraries for data generation and RNN development:
```python
	import numpy as np
	import torch
	import torch.nn as nn
	import torch.nn.functional as F
	from sklearn.utils import shuffle
```
#### Generate Synthetic Data
The first task is to generate the sequential integer sequences. This is achieved using the following function which will be explained.
```python
def sequence_generation(input_dim, samples):
    '''
    Generates basic integer sequences through rolling 
    Example: [1,2,3], [3,1,2], [2,3,1], etc.
    
    input_dim -> number of elements in sample
    samples -> no. samples in dataset
    '''
  
    _sequences = []
    _temp_seed_seq = np.arange(0,input_dim,1)
    
    for sample in range(samples):
        _sequences.append(np.roll(_temp_seed_seq,sample))
    
    # Labels are the last element in the sequence and features are the remaining elements.
    
    features = [seq[:-1] for seq in _sequences]  # Strip last element off of sequences, e.g. [[1], [2], [3]]
    features = [[[val] for val in seq] for seq in features]
    labels = [[seq[-1]] for seq in _sequences]  # Generates label by taking the last element off of the sequence, e.g. [4]
    
    # Shuffle Data
    features, labels = shuffle(features, labels, random_state=1337)
    
    features = torch.from_numpy(np.array(features))
    labels = torch.from_numpy(np.array(labels))
    
    return {'features':features, 'labels': labels}
```
Okay, the TLDR; of this function is that given an input dimension and number of samples, an output dataset int he form of a dictionary will be generated by rolling the arrays. For example,
```python
	sequence_generation(2,2)
```
Returns
```python
	{'features': tensor([[[0]],[[1]]], dtype=torch.int32),
	 'labels': tensor([[1],[0]], dtype=torch.int32)}
```
Using a utility function, we can make the output dictionary keys accessible via dot notation (x.*features*, x.*labels*):
```python
	class dotdict(dict):
	    """dot.notation access to dictionary attributes"""
	    __getattr__ = dict.get
	    __setattr__ = dict.__setitem__
	    __delattr__ = dict.__delitem__
```
Credit: *derek73* - https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary

We can now build training and test datasets for our basic RNN. We will be modelling sequences of 10 integers, e.g. [0,1,2,3,4,5,6,7,8,9] where we will split them into features: [0,1,2,3,4,5,6,7,8] and labels: [9]. Each element in the sequence is isolated in it's own array so that it can be fed into the RNN sequentially.
```python
	sequence_len = 10
	train_size = 1024
	test_size = 256
	train_data = dotdict(sequence_generation(sequence_len,train_size))
	test_data = dotdict(sequence_generation(sequence_len,test_size))
```
```python
	print(test_data.features[0])
	tensor([[7],
        [8],
        [9],
        [0],
        [1],
        [2],
        [3],
        [4],
        [5]], dtype=torch.int32)
```
```python
	print(test_data.labels[0])
	tensor([6], dtype=torch.int32)
```

#### Build Recurrent Neural Network in PyTorch
We will build the basic RNN using PyTorch. This RNN model will consist of an input layer, RNN block, and output layer (fully-connected).
```python
class RNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(RNN, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        
        self.rnn = nn.RNN(input_dim, hidden_dim)
        self.fc = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, x, h_0):
        output, h_n = self.rnn(x, h_0)
        
        output = output[-1, :, :]   # only pass last element to fc layer
        return self.fc(output.view(-1))
```
Defining model architecture and initialising model:
```python
# Model Architecture
input_dim = 1 #sequence_len-1
hidden_dim = 2
output_dim = sequence_len   # Output layer is encoded for each integer
learning_rate = 1e-3
epochs = 500
# Initialisation of Model
model = RNN(input_dim, hidden_dim, output_dim)
print(model)
```
Our RNN architecture
```python
RNN(
  (rnn): RNN(1, 2)
  (fc): Linear(in_features=2, out_features=10, bias=True)
)
```
Defining our loss criterion and optimiser:
```python
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
```
#### RNN Training
We will be training the RNN using our training set size of 1024 sequences. The process for training is as follows:
```python
for epoch in range(epochs):
    for idx, data in enumerate(train_data.features):
        X = data.view(data.shape[0],1,-1).float()    # need to reshape in terms of (seq_len, batch, input_size)
        
        y = train_data.labels[idx].long()

        h0 = torch.zeros((hidden_dim)).view(-1,1,hidden_dim).float()
        
        # -- forward pass ---
        output = model(X, h0)
        output = output.view(-1,output_dim)
        loss = criterion(output.float(), y)
        
        # -- backward pass and optimization --
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    if epoch % 10 == 0:
        print(f'Epoch: {epoch+1} - Loss: {loss:0.4f}')
```
Output:
```python
Epoch: 1 - Loss: 2.2262
Epoch: 11 - Loss: 1.4563
Epoch: 21 - Loss: 1.0373
Epoch: 31 - Loss: 0.8310
Epoch: 41 - Loss: 0.6756
Epoch: 51 - Loss: 0.5686
Epoch: 61 - Loss: 0.4995
Epoch: 71 - Loss: 0.4903
Epoch: 81 - Loss: 0.5504
Epoch: 91 - Loss: 0.6298
Epoch: 101 - Loss: 0.6735
Epoch: 111 - Loss: 0.6774
Epoch: 121 - Loss: 0.6650
Epoch: 131 - Loss: 0.7024
Epoch: 141 - Loss: 0.6646
Epoch: 151 - Loss: 0.5719
Epoch: 161 - Loss: 0.4871
Epoch: 171 - Loss: 0.4341
Epoch: 181 - Loss: 0.3868
Epoch: 191 - Loss: 0.3430
Epoch: 201 - Loss: 0.3032
Epoch: 211 - Loss: 0.2668
Epoch: 221 - Loss: 0.2237
Epoch: 231 - Loss: 0.2077
Epoch: 241 - Loss: 0.1799
Epoch: 251 - Loss: 0.1631
Epoch: 261 - Loss: 0.1439
Epoch: 271 - Loss: 0.1160
Epoch: 281 - Loss: 0.0964
Epoch: 291 - Loss: 0.0824
Epoch: 301 - Loss: 0.0698
Epoch: 311 - Loss: 0.0609
Epoch: 321 - Loss: 0.0407
Epoch: 331 - Loss: 0.0350
Epoch: 341 - Loss: 0.0353
Epoch: 351 - Loss: 0.0305
Epoch: 361 - Loss: 0.0217
Epoch: 371 - Loss: 0.0208
Epoch: 381 - Loss: 0.0174
Epoch: 391 - Loss: 0.0135
Epoch: 401 - Loss: 0.0086
Epoch: 411 - Loss: 0.0089
Epoch: 421 - Loss: 0.0089
Epoch: 431 - Loss: 0.0060
Epoch: 441 - Loss: 0.0085
Epoch: 451 - Loss: 0.0060
Epoch: 461 - Loss: 0.0036
Epoch: 471 - Loss: 0.0044
Epoch: 481 - Loss: 0.0050
Epoch: 491 - Loss: 0.0035
```
#### Testing Model
We evaluate our model on the synthetic test dataset of 256 sequences. The testing process is as follows:
```python
with torch.no_grad():
    correct = 0
    total = 0
    for idx, data in enumerate(test_data.features):
        X = data.view(data.shape[0],1,1).float()
        y = test_data.labels[idx].long()
        
        outputs = model(X, None)   # Don't worry about passing in any hidden information
        predicted = np.argmax(outputs.detach().numpy())   # get index of max value in output
        
        total += y.size(0)
        correct += (predicted == y).sum().item()
        if idx % 50 == 0:
            seq = [val[0][0] for val in X.data.numpy()]
            seqStr = '-'.join(str(int(val)) for val in seq)

            print(f'Seq: {seqStr} - P: {predicted} - T: {y.data.item()}')
            
    print(f'Test accuracy of the model on the test data: {correct / total * 100}%')
```
Output (P - predicted, T - Truth):
```python
Seq: 7-8-9-0-1-2-3-4-5 - P: 6 - T: 6
Seq: 0-1-2-3-4-5-6-7-8 - P: 8 - T: 9
Seq: 4-5-6-7-8-9-0-1-2 - P: 4 - T: 3
Seq: 7-8-9-0-1-2-3-4-5 - P: 6 - T: 6
Seq: 2-3-4-5-6-7-8-9-0 - P: 1 - T: 1
Seq: 3-4-5-6-7-8-9-0-1 - P: 2 - T: 2
Test accuracy of the model on the test data: 60.15625%
```


